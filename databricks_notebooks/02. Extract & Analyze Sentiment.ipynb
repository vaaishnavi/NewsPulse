{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e7f1e7-606b-45a9-bf4f-16e5dd23d2c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting databricks-genai-inference\n",
      "  Using cached databricks_genai_inference-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 867.6/867.6 kB 7.4 MB/s eta 0:00:00\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-2.12.1-py3-none-any.whl (20.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/20.2 MB 53.5 MB/s eta 0:00:00\n",
      "Collecting tenacity==8.2.3\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting databricks-sdk==0.19.1\n",
      "  Using cached databricks_sdk-0.19.1-py3-none-any.whl (447 kB)\n",
      "Collecting typing-extensions>=4.7.1\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests<3,>=2.26.0 in /databricks/python3/lib/python3.10/site-packages (from databricks-genai-inference) (2.28.1)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Collecting pydantic>=2.4.2\n",
      "  Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "Collecting pyyaml>=5.4.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Collecting google-auth~=2.0\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
      "  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Using cached langsmith-0.1.54-py3-none-any.whl (116 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.48\n",
      "  Using cached langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain) (1.21.5)\n",
      "Collecting langchain-community<0.1,>=0.0.36\n",
      "  Using cached langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
      "Collecting tiktoken<1,>=0.5.2\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 90.1 MB/s eta 0:00:00\n",
      "Collecting openai<2.0.0,>=1.24.0\n",
      "  Using cached openai-1.25.2-py3-none-any.whl (312 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.5.2)\n",
      "Requirement already satisfied: pandas<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.4.4)\n",
      "Collecting alembic!=1.10.0,<2\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.4/233.4 kB 33.4 MB/s eta 0:00:00\n",
      "Collecting gitpython<4,>=3.1.9\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 39.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging<25 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (21.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/lib/python3/dist-packages (from mlflow) (4.6.4)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.19.4)\n",
      "Collecting Flask<4\n",
      "  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 23.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2.11.3)\n",
      "Collecting cloudpickle<4\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (8.0.4)\n",
      "Requirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.9.1)\n",
      "Collecting graphene<4\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 28.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (8.0.0)\n",
      "Collecting docker<8,>=4.0.0\n",
      "  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.6/147.6 kB 25.0 MB/s eta 0:00:00\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.5.0-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.0/44.0 kB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.1.1)\n",
      "Collecting markdown<4,>=3.3\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 25.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz<2025 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2022.1)\n",
      "Collecting gunicorn<22\n",
      "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.2/80.2 kB 18.7 MB/s eta 0:00:00\n",
      "Collecting querystring-parser<2\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.8/78.8 kB 14.4 MB/s eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.11)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting blinker>=1.6.2\n",
      "  Downloading blinker-1.8.1-py3-none-any.whl (9.5 kB)\n",
      "Collecting Werkzeug>=3.0.0\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.3/227.3 kB 40.2 MB/s eta 0:00:00\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 18.5 MB/s eta 0:00:00\n",
      "Collecting Jinja2<4,>=2.11\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.3/133.3 kB 20.6 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 14.0 MB/s eta 0:00:00\n",
      "Collecting aniso8601<10,>=8\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 10.3 MB/s eta 0:00:00\n",
      "Collecting graphql-core<3.3,>=3.1\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202.9/202.9 kB 35.8 MB/s eta 0:00:00\n",
      "Collecting graphql-relay<3.3,>=3.1\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting anyio\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: idna in /databricks/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->databricks-genai-inference) (3.3)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->databricks-genai-inference) (2022.9.14)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting packaging<25\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Using cached orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.7.0)\n",
      "Collecting tqdm>4\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 17.7 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.18.2\n",
      "  Using cached pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.26.0->databricks-genai-inference) (2.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "Collecting regex>=2022.1.18\n",
      "  Using cached regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Using cached exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Installing collected packages: aniso8601, typing-extensions, tqdm, tenacity, sqlparse, sniffio, smmap, regex, querystring-parser, pyyaml, pyasn1, packaging, orjson, multidict, MarkupSafe, markdown, jsonpointer, itsdangerous, h11, greenlet, graphql-core, frozenlist, exceptiongroup, cloudpickle, click, cachetools, blinker, async-timeout, annotated-types, yarl, Werkzeug, typing-inspect, tiktoken, SQLAlchemy, rsa, pydantic-core, pyasn1-modules, marshmallow, Mako, jsonpatch, Jinja2, httpcore, gunicorn, graphql-relay, gitdb, docker, anyio, aiosignal, pydantic, httpx, graphene, google-auth, gitpython, Flask, dataclasses-json, alembic, aiohttp, openai, mlflow, langsmith, databricks-sdk, langchain-core, databricks-genai-inference, langchain-text-splitters, langchain_openai, langchain-community, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.1.0\n",
      "    Not uninstalling tenacity at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'tenacity'. No files were found to uninstall.\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Not uninstalling packaging at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'packaging'. No files were found to uninstall.\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.0.1\n",
      "    Not uninstalling markupsafe at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Not uninstalling click at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'click'. No files were found to uninstall.\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.4\n",
      "    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'blinker'. No files were found to uninstall.\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Not uninstalling jinja2 at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'Jinja2'. No files were found to uninstall.\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.6\n",
      "    Not uninstalling pydantic at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'pydantic'. No files were found to uninstall.\n",
      "  Attempting uninstall: databricks-sdk\n",
      "    Found existing installation: databricks-sdk 0.1.6\n",
      "    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-df797a0a-d4e3-4cba-8b25-45be6558cd12\n",
      "    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n",
      "Successfully installed Flask-3.0.3 Jinja2-3.1.4 Mako-1.3.3 MarkupSafe-2.1.5 SQLAlchemy-2.0.30 Werkzeug-3.0.3 aiohttp-3.9.5 aiosignal-1.3.1 alembic-1.13.1 aniso8601-9.0.1 annotated-types-0.6.0 anyio-4.3.0 async-timeout-4.0.3 blinker-1.8.1 cachetools-5.3.3 click-8.1.7 cloudpickle-3.0.0 databricks-genai-inference-0.2.3 databricks-sdk-0.19.1 dataclasses-json-0.6.5 docker-7.0.0 exceptiongroup-1.2.1 frozenlist-1.4.1 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.29.0 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 greenlet-3.0.3 gunicorn-21.2.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 itsdangerous-2.2.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.17 langchain-community-0.0.36 langchain-core-0.1.50 langchain-text-splitters-0.0.1 langchain_openai-0.1.6 langsmith-0.1.54 markdown-3.6 marshmallow-3.21.2 mlflow-2.12.1 multidict-6.0.5 openai-1.25.2 orjson-3.10.3 packaging-23.2 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.7.1 pydantic-core-2.18.2 pyyaml-6.0.1 querystring-parser-1.2.4 regex-2024.4.28 rsa-4.9 smmap-5.0.1 sniffio-1.3.1 sqlparse-0.5.0 tenacity-8.2.3 tiktoken-0.6.0 tqdm-4.66.4 typing-extensions-4.11.0 typing-inspect-0.9.0 yarl-1.9.4\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install databricks-genai-inference langchain langchain_openai mlflow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2417e071-bbff-4bf6-9cee-e13ea83556b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  ... sentiment\n",
      "0  https://www.msn.com/en-us/news/technology/tesl...  ...      None\n",
      "1  https://www.wired.com/story/zhidou-rainbow-ev-...  ...      None\n",
      "2  https://www.forbes.com/sites/brookecrothers/20...  ...      None\n",
      "3  https://www.msn.com/en-us/autos/news/tesla-is-...  ...      None\n",
      "4  https://www.msn.com/en-us/news/technology/tesl...  ...      None\n",
      "\n",
      "[5 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# load articles\n",
    "result = spark.sql(\"SELECT * FROM hackathon_schema.articles\")\n",
    "\n",
    "df = result.toPandas()\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4daec7-21c6-416b-9265-537d807b137d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Extract topic sentiment from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771a30f7-9133-4be5-9a68-2e91bb84d20a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'content', 'company_name', 'published_date', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3537261e-b2fb-4705-b904-4ccb50bc85b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
      "1    {\"layoffs\": null, \"restructuring\": 0.2, \"board...\n",
      "2    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
      "3    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
      "4    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Optional, Dict\n",
    "import time\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "model = ChatDatabricks(endpoint=\"databricks-dbrx-instruct\")\n",
    "\n",
    "# Define schema with topics\n",
    "class TopicSentimentSchema(BaseModel):\n",
    "    layoffs: Optional[float] = Field(description=\"Sentiment score for layoffs topic (-1 to 1 or null if not mentioned)\")\n",
    "    restructuring: Optional[float] = Field(description=\"Sentiment score for org restructuring topic (-1 to 1 or null if not mentioned)\")\n",
    "    board_changes: Optional[float] = Field(description=\"Sentiment score for board member departures or appointments topic (-1 to 1 or null if not mentioned)\")\n",
    "    mergers: Optional[float] = Field(description=\"Sentiment score for mergers or acquisitions topic (-1 to 1 or null if not mentioned)\")\n",
    "    investor_activity: Optional[float] = Field(description=\"Sentiment score for investor activity topic (-1 to 1 or null if not mentioned)\")\n",
    "    esg: Optional[float] = Field(description=\"Sentiment score for environmental, social, or governance issues (-1 to 1 or null if not mentioned)\")\n",
    "    revenue_growth: Optional[float] = Field(description=\"Sentiment score for revenue growth topic (-1 to 1 or null if not mentioned)\")\n",
    "    product_launches: Optional[float] = Field(description=\"Sentiment score for product launches topic (-1 to 1 or null if not mentioned)\")\n",
    "    expansion: Optional[float] = Field(description=\"Sentiment score for market expansion or contraction topic (-1 to 1 or null if not mentioned)\")\n",
    "    disputes: Optional[float] = Field(description=\"Sentiment score for legal disputes topic (-1 to 1 or null if not mentioned)\")\n",
    "    geo_political: Optional[float] = Field(description=\"Sentiment score for geo-political events topic (-1 to 1 or null if not mentioned)\")\n",
    "    macro_economic: Optional[float] = Field(description=\"Sentiment score for macro-economic events topic (-1 to 1 or null if not mentioned)\")\n",
    "    partnerships: Optional[float] = Field(description=\"Sentiment score for partnerships, contracts and deals topic (-1 to 1 or null if not mentioned)\")\n",
    "    cyber_security: Optional[float] = Field(description=\"Sentiment score for cyber security topic (-1 to 1 or null if not mentioned)\")\n",
    "    supply_chain: Optional[float] = Field(description=\"Sentiment score for supply chain topic (-1 to 1 or null if not mentioned)\")\n",
    "    labor_issues: Optional[float] = Field(description=\"Sentiment score for labor issues topic (-1 to 1 or null if not mentioned)\")\n",
    "    product_recalls: Optional[float] = Field(description=\"Sentiment score for product recalls topic (-1 to 1 or null if not mentioned)\")\n",
    "    overall_sentiment: Optional[float] = Field(description=\"Overall sentiment score for the article (-1 to 1 or null if not mentioned)\")\n",
    "\n",
    "\n",
    "def get_sentiment(schema, company, text):\n",
    "    # And a query to prompt LLM to populate the data structure\n",
    "    query = f\"\"\"\n",
    "    Analyze the news about {company} and return sentiment values for provided topics.\n",
    "    The sentiment should be defined based on whether it's good for a company and its shareholders (positive) or bad (negative).\n",
    "    The values should be between -1 for most negative sentiment and 1 for most positive sentiment.\n",
    "    0 for neutral sentiment. If a topic is not mentioned, the value should be null.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up a parser + inject instructions into the prompt template.\n",
    "    parser = PydanticOutputParser(pydantic_object=schema)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n{text}\\n\",\n",
    "        input_variables=[\"query\", \"text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "    response = chain.invoke({\"query\": query, \"text\": text})\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# iterate over rows\n",
    "for index, row in df.iterrows():\n",
    "    row_sentiment = get_sentiment(schema=TopicSentimentSchema, company=row['company_name'], text=row['content'])\n",
    "\n",
    "    # dump all content of row sentiment to new column in df\n",
    "    df.at[index, \"sentiment\"] = row_sentiment\n",
    "\n",
    "print(df['sentiment'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9fe11e-ebe3-4439-960c-74cde2a87606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "1     {\"layoffs\": null, \"restructuring\": 0.2, \"board...\n",
       "2     {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "3     {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "4     {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "5     {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "6     {\"layoffs\": -0.8, \"restructuring\": null, \"boar...\n",
       "7     {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "8     {\"layoffs\": -0.8, \"restructuring\": null, \"boar...\n",
       "9     {\"layoffs\": null, \"restructuring\": 0.5, \"board...\n",
       "10    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "11    {\"layoffs\": -0.8, \"restructuring\": null, \"boar...\n",
       "12    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "13    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "14    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "15    {\"layoffs\": -0.8, \"restructuring\": null, \"boar...\n",
       "16    {\"layoffs\": -0.8, \"restructuring\": -0.7, \"boar...\n",
       "17    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "18    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "19    {\"layoffs\": -0.8, \"restructuring\": null, \"boar...\n",
       "20    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "21    {\"layoffs\": -0.8, \"restructuring\": null, \"boar...\n",
       "22    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "23    {\"layoffs\": -0.7, \"restructuring\": null, \"boar...\n",
       "24    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "25    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "26    {\"layoffs\": null, \"restructuring\": null, \"boar...\n",
       "27    {\"layoffs\": -0.8, \"restructuring\": null, \"boar...\n",
       "28    {\"layoffs\": -0.8, \"restructuring\": null, \"boar...\n",
       "29    {\"layoffs\": null, \"restructuring\": 0.2, \"board...\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784cc925-609e-405d-8629-fb78485ffe5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the sentiment column to the table\n",
    "# alter_table_query = \"\"\"\n",
    "# ALTER TABLE hackathon_schema.articles\n",
    "# ADD COLUMNS (sentiment STRING)\n",
    "# \"\"\"\n",
    "# spark.sql(alter_table_query)\n",
    "\n",
    "# verify\n",
    "describe_table_query = \"DESCRIBE hackathon_schema.articles\"\n",
    "result = spark.sql(describe_table_query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99cda8b-16de-4e5f-b67c-0ccd288ee6e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------+--------------------+\n",
      "|                 url|             content|company_name|published_date|           sentiment|\n",
      "+--------------------+--------------------+------------+--------------+--------------------+\n",
      "|https://www.msn.c...|Tesla's 'apocalyp...|       Tesla|    2024-05-05|{\"layoffs\": null,...|\n",
      "|https://www.wired...|As Elon Musk Aban...|       Tesla|    2024-05-05|{\"layoffs\": null,...|\n",
      "|https://www.forbe...|Longer-Range Tesl...|       Tesla|    2024-05-05|{\"layoffs\": null,...|\n",
      "|https://www.msn.c...|Tesla is facing m...|       Tesla|    2024-05-05|{\"layoffs\": null,...|\n",
      "|https://www.msn.c...|Tesla's Optimus v...|       Tesla|    2024-05-05|{\"layoffs\": null,...|\n",
      "|https://www.msn.c...|Tesla plans to ch...|       Tesla|    2024-05-05|{\"layoffs\": null,...|\n",
      "|https://news.yaho...|Hyundai antes up ...|       Tesla|    2024-05-05|{\"layoffs\": -0.8,...|\n",
      "|https://www.msn.c...|These are the kin...|       Tesla|    2024-05-05|{\"layoffs\": null,...|\n",
      "|https://www.msn.c...|Musk just slashed...|       Tesla|    2024-05-04|{\"layoffs\": -0.8,...|\n",
      "|https://www.msn.c...|Is Musk Crazy Sma...|       Tesla|    2024-05-03|{\"layoffs\": null,...|\n",
      "|https://www.ft.co...|Can Elon Musk's T...|       Tesla|    2024-05-03|{\"layoffs\": null,...|\n",
      "|https://www.theda...|'I'm Still in Den...|       Tesla|    2024-05-04|{\"layoffs\": -0.8,...|\n",
      "|https://www.msn.c...|Have the wheels c...|       Tesla|    2024-05-03|{\"layoffs\": null,...|\n",
      "|https://www.forbe...|$299 Tesla Model ...|       Tesla|    2024-05-05|{\"layoffs\": null,...|\n",
      "|https://www.msn.c...|Tesla pushes to l...|       Tesla|    2024-05-04|{\"layoffs\": null,...|\n",
      "|https://www.msn.c...|Tesla's Superchar...|       Tesla|    2024-05-03|{\"layoffs\": -0.8,...|\n",
      "|https://arstechni...|What's happening ...|       Tesla|    2024-05-03|{\"layoffs\": -0.8,...|\n",
      "|https://www.msn.c...|How U.S. safety r...|       Tesla|    2024-05-04|{\"layoffs\": null,...|\n",
      "|https://www.msn.c...|He drove his fami...|       Tesla|    2024-05-03|{\"layoffs\": null,...|\n",
      "|https://www.msn.c...|Elimination of Te...|       Tesla|    2024-05-02|{\"layoffs\": -0.8,...|\n",
      "+--------------------+--------------------+------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# update articles table with sentiment col\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.createOrReplaceTempView(\"temp_articles\")\n",
    "\n",
    "# Run the merge query\n",
    "merge_query = \"\"\"\n",
    "MERGE INTO hackathon_schema.articles AS target\n",
    "USING temp_articles AS source\n",
    "ON target.url = source.url\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.sentiment = source.sentiment\n",
    "\"\"\"\n",
    "spark.sql(merge_query)\n",
    "\n",
    "# Verify the update\n",
    "result = spark.sql(\"SELECT * FROM hackathon_schema.articles\")\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02. Extract & Analyze Sentiment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
